# web服务器端口号
server.port=8080

# openAi配置项
#langchain4j.open-ai.chat-model.base-url=http://langchain4j.dev/demo/openai/v1
#langchain4j.open-ai.chat-model.api-key=demo
#langchain4j.open-ai.chat-model.model-name=gpt-4o-mini

# DeepSeek,实质上也使用了OpenAi的接口,只不过底层接入的大模型的配置参数发生了改变
#langchain4j.open-ai.chat-model.base-url=https://api.deepseek.com/v1
# 读取系统环境变量,如有修改,需重启IDEA
#langchain4j.open-ai.chat-model.api-key=${DEEPSEEK_API_KEY}
#langchain4j.open-ai.chat-model.model-name=deepseek-chat

# 集成百炼-deepseek
langchain4j.open-ai.chat-model.base-url=https://dashscope.aliyuncs.com/compatibleSTXmode/v1
# 这里写成固定值时，不需要加引号
langchain4j.open-ai.chat-model.api-key=${BAILIAN_API_KEY}
langchain4j.open-ai.chat-model.model-name=deepseek-v3
# 温度系数：取值范围通常在 0 到 1 之间。值越高，模型的输出越随机、富有创造性；
# 值越低，输出越确定、保守。这里设置为 0.9，意味着模型会有一定的随机性，生成的回复可能会比较多样化。
langchain4j.open-ai.chat-model.temperature=0.9

# ollama
langchain4j.ollama.chat-model.base-url=http://localhost:11434
langchain4j.ollama.chat-model.model-name=deepseek-r1:1.5b
langchain4j.ollama.chat-model.log-requests=true
langchain4j.ollama.chat-model.log-responses=true

# 阿里百炼平台
langchain4j.community.dashscope.chat-model.api-key=${BAILIAN_API_KEY}
langchain4j.community.dashscope.chat-model.model-name=qwen-max

# 这两个配置专用于LangChain4j+OpenAI的组合,如果使用其他模型或框架,需设置对应的日志控制机制
# 应用程序发送给大模型的请求日志响应日志
langchain4j.open-ai.chat-model.log-requests=true
langchain4j.open-ai.chat-model.log-responses=true
# 设置系统日志设置为debug级别
logging.level.root=debug

#MongoDB连接配置
spring.data.mongodb.uri=mongodb://localhost:27017/chat_memory_db